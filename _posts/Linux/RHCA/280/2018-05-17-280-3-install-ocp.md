---
layout: post
title:  "Install OCP(280-3)"
categories: Linux
tags: RHCA 280
---

### Preparing for the Installation

```
1. Open a Terminal (Applications → Favorites → Terminal) on the workstation host and then connect to the master host:

[student@workstation ~]$ ssh root@master

2. A DNS server is already configured to have a wildcard DNS A record. Remember that the wildcard record should point to the publicly available (external) IP address of the OpenShift route which will be on the node host. Identify the IP address of the node host and then test the wildcard record:

[root@master ~]# host node.lab.example.com
node.lab.example.com has address 172.25.250.11

[root@master ~]# ping -c1 yourName.cloudapps.lab.example.com
PING yourName.cloudapps.lab.example.com (172.25.250.11) 56(84) bytes of data.
64 bytes from node.lab.example.com (172.25.250.11): icmp_seq=1 ttl=64 time=2.57 ms
...Output omitted...

3. Verify that the master host is configured correctly for DNS name resolution.

    a. Verify the host name for the master host:

    [root@master ~]# hostname
    master.lab.example.com

    b. Take note of the master host's internal IP address:

    [root@master ~]# ip addr show dev eth0 | \
    grep "inet " | awk '{print $2}' | \
    cut -f1 -d/
    172.25.250.10

    c. Make sure the master host's internal DNS entry matches the internal IP address:

    [root@master ~]# host $(hostname)
    master.lab.example.com has address 172.25.250.10

4. Verify that the yum repositories are already configured:

[root@master ~]# yum repolist
Loaded plugins: langpacks, search-disabled-repos
...Output omitted...
repo id                                  repo name                                                 status
rhel-7-server-extras-rpms                Remote classroom copy of RHEL Extras                        393
rhel-7-server-optional-rpms              Remote classroom copy of Optional RHEL  RPMS              4,647
rhel-7-server-ose-3.4-rpms               Remote classroom copy of OCP RPMS                           553
rhel_dvd                                 Remote classroom copy of dvd                              4,751
repolist: 10,344

The repositories are configured in the /etc/yum.repos.d/training.repo file.

[root@master ~]# cat /etc/yum.repos.d/training.repo
[rhel-7-server-optional-rpms]
baseurl = http://content.example.com/ocp3.4/x86_64/rhelopt
enabled = true
gpgcheck = false
name = Remote classroom copy of Optional RHEL  RPMS

[rhel-7-server-extras-rpms]
baseurl = http://content.example.com/ocp3.4/x86_64/rhelextras
enabled = true
gpgcheck = false
name = Remote classroom copy of RHEL Extras

[rhel-7-server-ose-3.4-rpms]
baseurl = http://content.example.com/ocp3.4/x86_64/ocp
enabled = true
gpgcheck = false
name = Remote classroom copy of OCP RPMS

5. The OpenShift installer uses SSH to configure hosts. Generate and distribute the SSH keys:

    # ssh-keygen -f /root/.ssh/id_rsa -N ''
    # ssh-copy-id root@node.lab.example.com
    # ssh-copy-id root@master.lab.example.com

    Now try logging into the machine, with:   "ssh 'root@master.lab.example.com'"
    and check to make sure that only the key(s) you wanted were added.

6. The firewalld service must be stopped and disabled on both master and node hosts.

    # systemctl status firewalld
    # systemctl stop firewalld
    # systemctl disable firewalld

7. Add Trusted Root Certificate on both master and node hosts. The workstation.lab.example.com:5000 private registry uses a self-signed certificate.

    a. Verify that it is not possible to use the Registry API without adding the certificate as trusted:

    #  curl https://workstation.lab.example.com:5000/v2/openshift3/php-55-rhel7/manifests/latest
    curl: (60) Peer's certificate issuer has been marked as not trusted by the user.
    ...Output omitted...

    b. Add the certificate as trusted:

    # scp root@workstation.lab.example.com:/etc/pki/tls/certs/example.com.crt /etc/pki/ca-trust/source/anchors/
    # update-ca-trust extract


    c. Test again and observe that now the certificate is trusted by the server:

    # curl https://workstation.lab.example.com:5000/v2/openshift3/php-55-rhel7/manifests/latest
    {
       "schemaVersion": 1,
       "name": "openshift3/php-55-rhel7",
       "tag": "latest",
       "architecture": "amd64",
    ...
          "signature": "N5L82HQ3poEBWxCLxvPF..."
          "protected": "eyJmb3JtYXRMZW5ndGgi..."
        }
      ]
    }

8. Install and configure docker on both master and node hosts.

    a. install the docker package:

    # yum -y install docker

    b. Use a text editor such as vi to edit the /etc/sysconfig/docker file. Comment the line starting with ADD_REGISTRY and add two new lines as follows:

    #ADD_REGISTRY='--add-registry registry.access.redhat.com'
    ADD_REGISTRY='--add-registry workstation.lab.example.com:5000'
    BLOCK_REGISTRY='--block-registry docker.io --block-registry registry.access.redhat.com'

    > Warning: The BLOCK_REGISTRY line must be in a single contiguous line without line breaks.

    c. The default docker storage configuration uses loopback devices and is not appropriate for production. Red Hat considers the dm.thinpooldev storage option to be the only appropriate configuration for production use. If the docker service is currently running, you will need to stop the service and remove the default loopback Docker storage from the host:

    # systemctl stop docker.service
    # rm -rf /var/lib/docker/*

    d. In order to use dm.thinpooldev, the host must have space for an LVM thinpool available. The docker-storage-setup script will assist in configuring LVM. Additionally, the LVM cluster features must be disabled to allow the pool creation. A configuration file is provided for Docker storage setup. Verify the configuration file:

    # vim /etc/sysconfig/docker-storage-setup
    DEVS=/dev/vdb
    VG=docker-vg
    SETUP_LVM_THIN_POOL=yes

    e. Disable the LVM cluster features and run the docker-storage-setup command to use LVM:

    # lvmconf --disable-cluster
    #  docker-storage-setup
    ...Output omitted...
    INFO: Device node /dev/vdb1 exists.
      Physical volume "/dev/vdb1" successfully created.
      Volume group "docker-vg" successfully created
      Using default stripesize 64.00 KiB.
      Rounding up size to full physical extent 24.00 MiB
      Logical volume "docker-pool" created.
      Logical volume docker-vg/docker-pool changed.

    f. Examine the newly created logical volume docker-pool:

    # lvs /dev/docker-vg/docker-pool
    LV          VG        Attr       LSize Pool Origin Data%  Meta%    Move  Log  Cpy%Sync  Convert
    docker-pool docker-vg twi-a-t--- 7.95g            0.00   0.15

    g. Examine the docker storage configuration:

    # cat /etc/sysconfig/docker-storage
    DOCKER_STORAGE_OPTIONS="--storage-driver devicemapper --storage-opt dm.fs=xfs \
    --storage-opt dm.thinpooldev=/dev/mapper/docker--vg-docker--pool \
    --storage-opt dm.use_deferred_removal=true "

    h. Start and enable the docker service.

    # systemctl start docker; systemctl enable docker

    i. Verify that docker can pull images from the private registry:

    # docker pull rhel7
    Using default tag: latest
    Trying to pull repository workstation.lab.example.com:5000/rhel7 ...
    latest: Pulling from workstation.lab.example.com:5000/rhel7
    ...

```

### Installing Packages and Fetching Images

Packages:
atomic-openshift-docker-excluder
atomic-openshift-excluder
atomic-openshift-utils
bind-utils
bridge-utils
git
iptables-services
net-tools
wget

Here is a minimal list of OCP container images to download using docker pull:

*    openshift3/ose-haproxy-router
*    openshift3/ose-deployer
*    openshift3/ose-sti-builder
*    openshift3/ose-pod
*    openshift3/ose-docker-registry
*    openshift3/ose-docker-builder
*    openshift3/registry-console

The images listed above are for internal OCP services. Optionally, you can also download images for application runtimes, such as:

*    openshift3/ruby-20-rhel7
*    openshift3/mysql-55-rhel7
*    openshift3/php-55-rhel7
*    jboss-eap-6/eap64-openshift
*    openshift3/nodejs-010-rhel7

```
1. Install the required software on the master and node:
# yum -y install atomic-openshift-docker-excluder atomic-openshift-excluder atomic-openshift-utils bind-utils bridge-utils git iptables-services net-tools wget
2. Download images
# docker images
# docker pull ...
```

### Running the Installer

```
1. Remove the openshift package exclusions from the master and node hosts:

# atomic-openshift-excluder unexclude
```

2. During the installation of OCP, the changes previously made to docker's configuration will be lost. Make a backup of /etc/sysconfig/docker on both the master and node hosts so the file's configuration can be easily restored:

# cp /etc/sysconfig/docker /etc/sysconfig/docker-class

3. Interactively run the OpenShift Container Platform installer on the master node. Enter y at the Are you ready to continue? prompt.

[root@master ~]# atomic-openshift-installer install
Are you ready to continue? [y/N]: y

4. The installer asks the user to connect to remote hosts. Press Enter to continue.

User for ssh access [root]: Enter

5. The installers asks if you want to install OCP or a standalone registry. Press Enter to accept the default value of 1, which installs OCP.

Which variant would you like to install?
(1) OpenShift Container Platform
(2) Registry
Choose a variant from above:  [1]: Enter

6. The installer prompts you for details about the master node. Enter master.lab.example.com as the hostname of master, Enter y to confirm that this host will be the master, and press Enter to accept the default rpm option.

*** Host Configuration ***
Enter hostname or IP address: master.lab.example.com
Will this host be an OpenShift master? [y/N]: y
Will this host be RPM or Container based (rpm/container)? [rpm]: Enter

7. You have added details for the OCP master. You also need to add an OCP node. Enter y in the Do you want to add additional hosts? prompt, enter node.lab.example.com as the hostname of the node, Enter N to confirm that this host will not be the master, and press Enter to accept the default rpm option.

*** Installation Summary ***
Hosts:
- master.lab.example.com
  - OpenShift master
  - OpenShift node
  - Etcd

Total OpenShift masters: 1
Total OpenShift nodes: 1

NOTE: Add a total of 3 or more masters to perform an HA installation.

Do you want to add additional hosts? [y/N]: y
Enter hostname or IP address: node.lab.example.com
Will this host be an OpenShift master? [y/N]: N
Will this host be RPM or Container based (rpm/container)? [rpm]: Enter

8. The OpenShift cluster will have only two hosts. Enter N at the Do you want to add additional hosts? prompt.

*** Installation Summary ***
Hosts:
- master.lab.example.com
  - OpenShift master
  - OpenShift node (Unscheduled)
  - Etcd
- node.lab.example.com
  - OpenShift node (Dedicated)

Total OpenShift masters: 1
Total OpenShift nodes: 2

NOTE: Add a total of 3 or more masters to perform an HA installation.
Do you want to add additional hosts? [y/N]: N

9. The installer asks if you want to override the cluster host name. Press Enter to accept the default value of None.

You have chosen to install a single master cluster (non-HA).
In a single master cluster, the cluster host name (Ansible variable openshift_master_cluster_public_hostname) is set by default to the host name of the single master. In a multiple master (HA) cluster, the FQDN of a host must be provided that will be configured as a proxy. This could be either an existing load balancer configured to balance all masters on
port 8443 or a new host that would have HAProxy installed on it.

(Optional)
If you want to override the cluster host name now to something other than the default (the host name of the single master), or if you think you might add masters later to become an HA cluster and want to future proof your cluster host name choice, please provide a FQDN. Otherwise, press ENTER to continue and accept the default.

Enter hostname or IP address [None]: Enter

10. The installer prompts you for a host where the storage for the OCP registry will be configured. Press Enter to accept the default value of master.lab.example.com.

Setting up high-availability masters requires a storage host. Please provide a host that will be configured as a Registry Storage.
Note: Containerized storage hosts are not currently supported.
Enter hostname or IP address [master.lab.example.com]: Enter

11. Enter cloudapps.lab.example.com as the DNS sub-domain for the OCP router.

You might want to override the default subdomain used for exposed routes. If you don't know what this is, use the default value.
New default subdomain (ENTER for none) []: cloudapps.lab.example.com

12. Accept the default value of none for both the http and https proxy.

If a proxy is needed to reach HTTP and HTTPS traffic, please enter the
name below. This proxy will be configured by default for all processes
that need to reach systems outside the cluster. An example proxy value
would be:
    http://proxy.example.com:8080/
More advanced configuration is possible if using Ansible directly:
https://docs.openshift.com/enterprise/latest/install_config/http_proxies.html
Specify your http proxy ? (ENTER for none) []: Enter
Specify your https proxy ? (ENTER for none) []: Enter

13. The installer prints a final summary based on your input and asks for confirmation. Ensure that the hostname and IP address details of master and node hosts are correct, and then enter y to continue.

Do the above facts look correct? [y/N]: y

14. Enter y to start the installation.

Are you ready to continue? [y/N]: y
...
The installation was successful!

If this is your first time installing please take a look at the Administrator Guide for advanced options related to routing, storage, authentication, and more:

http://docs.openshift.com/enterprise/latest/admin_guide/overview.html

```

### Completing Postinstallation Tasks

```
1. After the installation completes, recover docker's configuration on both the master and node hosts, and then restart the docker service:

# cp /etc/sysconfig/docker-class /etc/sysconfig/docker
cp: overwrite '/etc/sysconfig/docker'? yes
# systemctl restart docker

2. Reinstate OCP package exclusions on both the master and node hosts:

# atomic-openshift-excluder exclude

3. Check the status of the OCP nodes from the master host:

[root@master ~]# oc get nodes
NAME                    STATUS                   AGE
master.lab.example.com  Ready,SchedulingDisabled 9m
node.lab.example.com    Ready                    9m

> By default, the master node is unschedulable. New pods are blocked from being scheduled to start on any node that has a status of SchedulingDisabled. 

4. Check the status of the pods that were created during the OCP installation:

[root@master ~]# oc get pods
NAME                        READY     STATUS             RESTARTS   AGE
docker-registry-6-oytdi     1/1       Running            0          1m
registry-console-1-deploy   1/1       Running            0          20m
registry-console-1-vdj8x    0/1       ImagePullBackOff   0          1m
router-1-7n637              1/1       Running            0          1m

5. The registry-console pod does not have a status of Running because the default configuration of the OCP installer tries to pull the registry-console image from registry.access.redhat.com. It may have a status of ImagePullBackOff, ErrImagePull, or Error. Modify the deployment configuration for the registry console to point to workstation.lab.example.com:5000, and then verify that all pods are running:

[root@master ~]# oc edit dc registry-console

6. Verify that all pods move to a Running status:

[root@master ~]# oc get pods
NAME                       READY     STATUS    RESTARTS   AGE
docker-registry-6-oytdi    1/1       Running   0          1m
registry-console-2-wijvb   1/1       Running   0          20s
router-1-7n637             1/1       Running   0          1m

7. Verify that the OpenShift services are running. The master host has both the atomic-openshift-master and atomic-openshift-node services started and enabled. The node host has only the atomic-openshift-node service started and enabled.

[root@master ~]# systemctl status atomic-openshift-master
  atomic-openshift-master.service - Atomic OpenShift Master
   Loaded: loaded (/usr/lib/systemd/system/atomic-openshift-master.service, enabled; vendor preset: disabled)
   Active: active (running) since Tue 2017-01-31 01:37:32 EST; 22min ago
...Output omitted...
[root@master ~]# systemctl status atomic-openshift-node
  atomic-openshift-node.service - Atomic OpenShift Node
   Loaded: loaded (/usr/lib/systemd/system/atomic-openshift-node.service, enabled; vendor preset: disabled)
   Active: active (running) since Tue 2017-01-31 01:47:20 EST; 16min ago
...Output omitted...

[root@node ~]# systemctl status atomic-openshift-node
  atomic-openshift-node.service - Atomic OpenShift Node
   Loaded: loaded (/usr/lib/systemd/system/atomic-openshift-node.service, enabled; vendor preset: disabled)
   Active: active (running) since Tue 2017-01-31 01:47:20 EST; 16min ago
...Output omitted...

8. Verify that OCP can start a new pod. The pod is created from the openshift/hello-openshift image on the classroom private registry and run on port 8080 inside the container.

    a. Run the following command to create the pod and associated resources:

    [root@master ~]# oc new-app --docker-image=workstation.lab.example.com:5000/openshift/hello-openshift
    ...
    --> Creating resources ...
    ...
    --> Success
        Run 'oc status' to view your app

    b. Monitor the pod being created and started. The openshift/hello-openshift image is searched and downloaded (pulled) by OCP from the known container registries, and after the image is available, the pod and container will be created. Run the following command from the master host:

    [root@master ~]# watch oc get pods
    NAME                      READY   REASON             RESTARTS  AGE
    docker-registry-6-5ew5e   1/1     Running            0         18m
    hello-openshift-1-nm2n9   0/1     ContainerCreating  0         4s
    registry-console-1-f3vu1  1/1     Running            0         18m
    router-1-taau8            1/1     Running            0         18m

    Notice the pod name gets a random suffix.

    c. Wait until the output changes from ContainerCreating to Running, and then escape from the watch command by pressing Ctrl+C. Because this pod starts so quickly, it is possible that you will not see the ContainerCreating state.

    NAME                      READY   REASON
    docker-registry-6-5ew5e   1/1     Running  0         18m
    hello-openshift-1-nm2n9   1/1     Running  0         6s
    registry-console-1-f3vu1  1/1     Running  0         18m
    router-1-taau8            1/1     Running  0         18m
    ^C

    d. Confirm that the hello-openshift-* container is running on the node, and not on the master. Take note of the IP value in the pod description. This is the pod's internal dynamic IP address:

    [root@master ~]# oc describe pod hello-openshift-1-nm2n9 | head
    Name:                 hello-openshift-1-nm2n9
    Namespace:            default
    Security Policy:      restricted
    Node:                 node.lab.example.com/172.25.250.11
    Start Time:           Tue, 31 Jan 2017 22:17:16 +0530
    Labels:               app=hello-openshift
    ...
    Status:               Running
    IP:                   10.129.0.7
    ...
    
9. Verify that the hello-openshift application is running and that the exposed routes for applications resolve to the default DNS wildcard subdomain that the OCP installer configured during installation.

    a. From the master host, try to access the application using the pod's internal IP address obtained in the previous step and the container port (8080):

    [root@master ~]# curl http://10.129.0.7:8080
    Hello OpenShift!

    b. Expose the pod's service for external access:

    [root@master ~]# oc expose svc hello-openshift
    route "hello-openshift" exposed

    c. Verify that the default router pod accepts requests for the pod using the DNS wildcard domain:

    [root@master ~]# curl http://hello-openshift-default.cloudapps.lab.example.com
    Hello OpenShift!

    d. The quick installation method prompted you to override the default subdomain used for exposed routes. Verify that the default subdomain was correctly set:

    [root@master ~]# grep subdomain /etc/origin/master/master-config.yaml
          subdomain:  "cloudapps.lab.example.com"

    e. Delete the hello-openshift pod and associated resources:

    [root@master ~]# oc delete all -l app=hello-openshift
    imagesgtream "hello-openshift" deleted
    deploymentconfig "hello-openshift" deleted
    route "hello-openshift" deleted
    service "hello-openshift" deleted
    pod "hello-openshift-1-nm2n9" deleted

10. Modify the existing ImageStream resources.

The OpenShift Container Platform installer configures a number of ImageStream resources, which are used by Source To Image (S2I) builders, to pull images from the Red Hat subscriber private registry. In an environment without Internet access, the ImageStreams should pull the images from a private registry.

The default image stream files are located at /usr/share/openshift/examples/image-streams/image-streams-rhel7.json, and /usr/share/openshift/examples/xpaas-streams/jboss-image-streams.json

To customize OCP to use a private registry, the following commands should be executed on the master:

[root@master ~]# oc get is -n openshift
[root@master ~]# oc delete is -n openshift --all
[root@master ~]# oc create -f /root/DO280/labs/install/image-streams-rhel7.json -n openshift
[root@master ~]# oc create -f /root/DO280/labs/install/jboss-image-streams.json -n openshift
[root@master ~]# oc get is -n openshift

https://github.com/openshift/origin/blob/release-3.6/examples/image-streams/image-streams-centos7.json
https://github.com/openshift/origin/blob/release-3.6/examples/image-streams/image-streams-rhel7.json
```


```
# docker version
# kubectl version
# openshift version

# oc get project   
# kubectl get namespace

# docker-registry-cli workstation.lab.example.com:5000 list all ssl
# docker-registry-cli workstation.lab.example.com:5000 search mysql ssl
```

### Configuring Authentication

*    /etc/origin/master/master-config.yaml (master)
*    /etc/origin/openshift-passwd (master)
*    Application URL 	https://master.lab.example.com:8443

```
1. Usernames and passwords for the authentication mechanism are configured using the htpasswd binary. This utility is available by installing the httpd-tools package. On the master host, install this package:

[root@master ~]# yum -y install httpd-tools

2. Configure the OCP master to authenticate using an Apache web server htpasswd file.

    a. Edit the /etc/origin/master/master-config.yaml file to configure the htpasswd authentication mechanism.

    [root@master ~]# vim /etc/origin/master/master-config.yaml

    b. Find the oauthConfig section. By default, OCP is configured to deny all login attempts.

    ...Output omitted...
    oauthConfig:
      assetPublicURL: https://master.lab.example.com:8443/console/
      grantConfig:
        method: auto
      identityProviders:
      - challenge: true
        login: true
        mappingMethod: claim
        name: deny_all
        provider:
          apiVersion: v1
          kind: DenyAllPasswordIdentityProvider
    ...Output omitted...

    c. Replace DenyAllPasswordIdentityProvider with HTPasswdPasswordIdentityProvider in the kind attribute.

    ...Output omitted...
    oauthConfig:
      assetPublicURL: https://master.lab.example.com:8443/console/
      grantConfig:
        method: auto
      identityProviders:
      - challenge: true
        login: true
        mappingMethod: claim
        name: deny_all
        provider:
          apiVersion: v1
          kind: HTPasswdPasswordIdentityProvider
    ...Output omitted...

    d. Add an attribute named file pointing to /etc/origin/openshift-passwd. This line must go directly above or below the line that we previously modified and it must be indented at the same level. Use :set ai to enable auto-indenting while you edit the file.

    ...
    oauthConfig:
      assetPublicURL: https://master.lab.example.com:8443/console/
      grantConfig:
        method: auto
      identityProviders:
      - challenge: true
        login: true
        mappingMethod: claim
        name: deny_all
        provider:
          apiVersion: v1
          file: /etc/origin/openshift-passwd
          kind: HTPasswdPasswordIdentityProvider
    ...Output omitted...

    e. Save the changes and exit the editor. 

3. Add user account.

    a. Create the /etc/origin/openshift-passwd file:

    [root@master ~]# touch /etc/origin/openshift-passwd

    b. Create an OCP user with the username of student and a password of redhat:

    [root@master ~]# htpasswd -b /etc/origin/openshift-passwd student redhat
    Adding password for user student

    c. Restart the atomic-openshift-master service:

    [root@master ~]# systemctl restart atomic-openshift-master

4. Log in as the student user. Open a web browser on the workstation host and navigate to the following URL: https://master.lab.example.com:8443.
5. The master host uses a self-signed certificate. Click Advanced and then click Add Exception to add an exception allowing this connection:
6. Verify that the Permanently store this exception option is selected, then click Confirm Security Exception. 
7. Login to the OCP master web console using student as the Username and redhat as the Password.

```


